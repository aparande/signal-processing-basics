{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranking-castle",
   "metadata": {},
   "source": [
    "# Basics of Signal Processing\n",
    "**Authors**: Anmol Parande, Hoang Nguyen, Jordan Grelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-brass",
   "metadata": {},
   "source": [
    "Throughout this notebook, we will be working with a clip from [Suzanne Vega's song, \"Tom's Diner\"](https://www.youtube.com/watch?v=FLP6QluMlrg). We will use `scipy` to read the audio data from the `.wav` file. It will return the sampling frequency `fs` as well as the audio samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, audio = wavfile.read(\"toms-diner.wav\")\n",
    "print(f\"Loaded {audio.size} samples at a sampling rate of {fs}Hz\")\n",
    "ipd.Audio(audio, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-lease",
   "metadata": {},
   "source": [
    "# I. Time Domain Filtering - Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-professional",
   "metadata": {},
   "source": [
    "## I.a Linear Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-tactics",
   "metadata": {},
   "source": [
    "## I.b Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-flesh",
   "metadata": {},
   "source": [
    "## I.c Nonlinear Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-rental",
   "metadata": {},
   "source": [
    "# II. DFT - Anmol\n",
    "\n",
    "Typically, when we look at signals, we look at them in the so-called time-domain. Each sample $x[k]$ represents the amplitude of the signal at time-step $k$. This tells us what the signal looks like. One question we might want to ask ourselves is _\"How fast is the signal changing?\"_\n",
    "\n",
    "For sinusoidal signals like $x[n] = \\cos(\\omega n)$ and $x[n] = \\sin(\\omega n)$, answering this question is easy because a larger $\\omega$ means the signal is changing faster ($\\omega$ is known as the angular frequency). For example, consider the plots below which each consist of 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 100, 100)\n",
    "slow_cos = np.cos(2 * np.pi * n / 100)\n",
    "fast_cos = np.cos(2 * np.pi * 5 * n / 100)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.stem(n, slow_cos, use_line_collection=True)\n",
    "plt.title(\"$\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)$\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"$\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)$\")\n",
    "plt.stem(n, fast_cos, use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-paper",
   "metadata": {},
   "source": [
    "$\\cos\\left(\\frac{10\\pi}{100} t\\right)$ is clearly changing a lot faster. If we allow ourselves to consider complex signals, then we can generalized sinusoids using the complex exponential $e^{j\\omega}$. Just like real sinusoids, the $\\omega$ in the signal $x[n] = e^{j\\omega n}$ determines how fast the signal changes (i.e rotates around the unit circle). If we can somehow \"project\" our time-domain signal $x[n]$ onto a \"basis\" of complex exponential signals, then, then the coefficients $X[k]$ should tell us how much the signal changes.\n",
    "\n",
    "The Discrete Fourier Transform is the change of basis which we use for a finite, length-$N$ signal to understand how fast it is changing. The basis used in the DFT are the $N$th roots of unity (i.e the complex solutions to $\\omega=1$). More specifically, the $k$th basis vector is given by $\\phi_k[n] = e^{j\\frac{2\\pi}{N}kn}$. Using the complex inner product $\\langle \\vec{x}, \\vec{y} \\rangle = \\vec{y}^*\\vec{x}$, the DFT coefficients are given by\n",
    "\n",
    "$$X[k] = \\langle x, \\phi_k \\rangle = \\sum_{n=0}^{N-1}x[n]e^{-j\\frac{2\\pi}{N}kn}$$.\n",
    "\n",
    "From the DFT coefficients, we can recover the time-domain coefficients using the inverse DFT.\n",
    "\n",
    "$$x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1}X[k]e^{j\\frac{2\\pi}{N}kn}$$.\n",
    "\n",
    "There are many ways to compute the DFT. The fastest method is the Fast Fourier Transform (FFT), which is an algorithm which computes the DFT. It is built into `numpy` as part of the `fft` submodule.\n",
    "\n",
    "If we look at the DFT coefficients of the two cosines we saw earlier, we can see that it is indeed doing exactly what we wanted it to: characterizing the frequency of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_cos_fft = np.fft.fft(slow_cos)\n",
    "fast_cos_fft = np.fft.fft(fast_cos)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.stem(n, np.abs(slow_cos_fft), use_line_collection=True)\n",
    "plt.title(\"$|DFT\\{\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)\\}|$\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"$|DFT\\{\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)\\}|$\")\n",
    "plt.stem(n, np.abs(fast_cos_fft), use_line_collection=True)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.stem(n, np.angle(slow_cos_fft), use_line_collection=True)\n",
    "plt.title(\"$\\\\arg \\\\left(DFT\\{\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)\\}\\\\right)$\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"$\\\\arg \\\\left(DFT\\{\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)\\}\\\\right)$\")\n",
    "plt.stem(n, np.angle(fast_cos_fft), use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-soundtrack",
   "metadata": {},
   "source": [
    "Since $\\cos\\left(\\frac{2\\pi}{100}n\\right) = \\frac{1}{2}\\left(e^{j\\frac{2\\pi}{100}n} + e^{-j\\frac{2\\pi}{100}n}\\right)$, we should expect peaks at $k = 1$ and $k =-1$ (note that because the roots of unity are periodic, $k=-1$ is the same basis vector as $k=99$). Likewise, since $\\cos\\left(\\frac{10\\pi}{100}n\\right) = \\frac{1}{2}\\left(e^{j\\frac{10\\pi}{100}n} + e^{-j\\frac{10\\pi}{100}n}\\right)$, we should expect peaks at $k=5$ and $k=-5$.\n",
    "\n",
    "There are a few things to note:\n",
    "1. The DFT coefficients are complex numbers, so we need both magnitude (top plots) and phase (bottom plots) to characterize the signal information\n",
    "2. For both $\\cos\\left(\\frac{2\\pi}{100}n\\right)$ and $\\cos\\left(\\frac{10\\pi}{100}n\\right)$, we should only expect 2 non-zero coefficients. However, we have apparently many non-zero coefficients. These are due to numerical instability in the DFT algorithm (if you print them out, these coefficients are on the order of $10^{-3}$ in magnitude and so are insignificant).\n",
    "3. The DFT basis is **not** orthonormal. This is why we must scale by $\\frac{1}{N}$ when applying the inverse DFT (`np.fft.ifft` in numpy). This is also why the peak magnitudes of the example signals above are 50 and not $\\frac{1}{2}$.\n",
    "4. DFT basis vectors are complex conjugates of each other (i.e $\\phi_k[n] = \\phi_{N-k}[n]^*$). This means for real signals, $X[k] = X^*[N-k]$.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-portsmouth",
   "metadata": {},
   "source": [
    "To get a better feel for the DFT, compute and plot the magnitude of the DFT coefficients of our clip from Tom's Diner in decibels ($dB = 20\\log_{10}(X[k])$). Since our song is a real signal, do not plot the complex conjugate coefficients since they are redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "song_dft = 20 * np.log10(np.abs(np.fft.fft(audio)))\n",
    "plt.plot(song_dft[:audio.size // 2]) # Coefficents N/2 to N are complex coefficients\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-memorabilia",
   "metadata": {},
   "source": [
    "**Comprehension Question**: Do you notice anything interesting about the chart above?\n",
    "\n",
    "**Answer**: Around index 150,000, there is a sharp decline in the magnitude of the DFT coefficients. It turns out that this DFT coefficient represents approximately 12.5 kHz (we'll see how to compute this later), which is close to the human hearing limit of about 20kHz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-canadian",
   "metadata": {},
   "source": [
    "**Comprehension Question**: What does the first coefficient $X[0]$ of the DFT represent in simple terms?\n",
    "\n",
    "**Answer**: It is the sum of the signal (we can see this from the formula by letting $k=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-salem",
   "metadata": {},
   "source": [
    "## II.a PSD\n",
    "\n",
    "In signal processing, due to noise, numerical stability, and other issues, we often care about the dominant frequencies in the signal (e.g when we are looking for formants in a vowel). This means we want to look at the magnitude of the DFT coefficients. However, sometimes peaks in the DFT are difficult to distinguish when looking at a magnitude plot. To better distinguish peaks, we can instead look at $|X[k]|^2$, the so-called **Power Spectral Density (PSD)**.\n",
    "\n",
    "The Power Spectral Density is the essentially the magnitude of the DFT of the auto-correlation of the signal $x$. This is because when $x[n]$ has DFT coefficients $X[k]$, then $x[-n]$ has DFT coefficients $X^*[k]$ and since auto-correlation is the convolution of $x[n] * x[-n]$, and convolution in the time-domain is multiplication in the frequency domain, $PSD = X[k] X^*[k] = |X[k]|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-adaptation",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the PSD to guess what vowels these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_1 = audio[int(7.25 * fs):int(fs * 7.45)] # ai\n",
    "vowel_2 = audio[int(11.45 * fs):int(11.70 * fs)] # e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "vowel_1_dft = np.abs(np.fft.fft(vowel_1)) ** 2\n",
    "plt.plot(vowel_1_dft[:200]) # Coefficents N/2 to N are complex coefficients\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# ** YOUR CODE HERE ** #\n",
    "vowel_2_dft = np.abs(np.fft.fft(vowel_2)) ** 2\n",
    "plt.plot(vowel_2_dft[:200]) # Coefficents N/2 to N are complex coefficients\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-filter",
   "metadata": {},
   "source": [
    "# III. Frequency Domain Filtering - Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-andrew",
   "metadata": {},
   "source": [
    "# IV. Sampling Theory - Anmol\n",
    "\n",
    "In the real-world, most signals are continuous (i.e they are functions from $\\mathbb{R}\\to\\mathbb{R}$). Meanwhile, computers operate in the discrete space (i.e they are functions from $\\mathbb{N}\\to\\mathbb{R}$. This means that in order to analyze any continuous signal, we need to somehow discretize it so it can be stored in finite memory.\n",
    "\n",
    "Given a continuous signal $x_c(t)$, we can obtain a discrete signal by letting $x_d[n] = x_c(f(n))$ where $f: \\mathbb{N}\\to\\mathbb{R}$ describes our sampling scheme.\n",
    "\n",
    "A **uniform, non-adaptive sampling** scheme is where we pick some sampling frequency $\\omega_s$ and let $f(n) = \\frac{n}{\\omega_s}$. We can think of it as \"saving\" the value of the continuous time signal every $\\frac{1}{\\omega_s}$ seconds. _Uniform_ means that $\\omega_s$ is constant (i.e it does not depend on $n$), and _non-adaptive_ means $\\omega_s$ is independent of the samples we have seen so far. Uniform, non-adaptive sampling schemes are what we most frequently use for sampling because of their simplicity and well-known theoeretical guarantees. For the rest of the notebook, we will assume all sampling is uniform and non-adaptive.\n",
    "\n",
    "Because sampling has the potential to destroy information, we need to understand how it impacts the frequency domain. In continuous time, frequencies exist on the range $[0, \\infty)$. However, in discrete time, the fastest that a signal can change is $\\pi$ radians / sample (i.e alternating from 1 to -1 like $\\cos(\\pi n)$). When we take the DFT of a signal that we sampled, we want to know how our angular frequencies relate to the continuous frequencies.\n",
    "\n",
    "The easiest way to think of how continuous frequencies relate to discrete frequencies is by mapping the interval $\\left[0, \\frac{f_s}{2}\\right]$ (continuous frequencies) to the interval $[0, \\pi]$ (angular frequencies). Given an angular frequency $\\omega_d\\in[0, \\pi]$, the continuous frequency that it represent $\\omega_c = \\frac{f_s}{2\\pi}\\omega_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-latitude",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Plot the magnitude of DFT coefficients (in decibels) of our clip from Tom's Diner and label the x-axis with the continuous time frequencies. Ignore the complex conjugate coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "freqs = np.linspace(0, fs / 2, audio.size // 2)\n",
    "song_dft = 20 * np.log10(np.abs(np.fft.fft(audio)))\n",
    "plt.plot(freqs, song_dft[:audio.size // 2]) # Coefficents N/2 to N are complex coefficients\n",
    "plt.xlabel(\"Hz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-mambo",
   "metadata": {},
   "source": [
    "## IV.a Aliasing\n",
    "\n",
    "How frequently we sample matters a lot. If we sample too slowly, then we lose information. If we sample too fast, then we are wasting memory. The three plots below are all samples of a 10 second long sine wave $x(t) = \\sin(2\\pi t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "hundred_hz = np.linspace(0, 10, 1000)\n",
    "ten_hz = np.linspace(0, 10, 100)\n",
    "one_hz = np.linspace(0, 10, 10)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(one_hz, np.sin(2 * np.pi * one_hz))\n",
    "plt.title(\"$f_s$ = 1Hz\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ten_hz, np.sin(2 * np.pi * ten_hz))\n",
    "plt.title(\"$f_s$ = 10Hz\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(hundred_hz, np.sin(2 * np.pi * hundred_hz))\n",
    "plt.title(\"$f_s$ = 100Hz\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-platform",
   "metadata": {},
   "source": [
    "Notice how the faster sampling frequencies 10Hz and 100Hz look virtually identical and cycle 10 times in 10 seconds as we expect a 1Hz sine wave to do. However, when we sample at 1Hz, our samples look like they came from a 0.1Hz sine wave, not a 1Hz sine wave. When higher frequencies \"masquerade\" as lower frequencies, this is known as **aliasing**. The effects of aliasing are very clear in the frequency domain through the following example where we sample the signal $x_c(t) = \\sin(2\\pi t) + \\sin(2\\pi * 10t)$ with a sampling frequency of 11Hz vs a sampling frequency of 50Hz over the course of 1 second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_c(t):\n",
    "    return np.sin(2 * np.pi * t) + np.sin(2 * np.pi * 10 * t)\n",
    "\n",
    "eleven_hz = np.linspace(0, 1, 11)\n",
    "fifty_hz = np.linspace(0, 1, 50)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(eleven_hz, x_c(eleven_hz))\n",
    "plt.title(\"$f_s$ = 11Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(fifty_hz, x_c(fifty_hz))\n",
    "plt.title(\"$f_s$ = 50Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(np.linspace(0, 11, eleven_hz.size), np.abs(np.fft.fft(x_c(eleven_hz))))\n",
    "plt.title(\"$f_s$ = 11Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(np.linspace(0, 50, fifty_hz.size), np.abs(np.fft.fft(x_c(fifty_hz))))\n",
    "plt.title(\"$f_s$ = 50Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-inside",
   "metadata": {},
   "source": [
    "When we sampled at 50Hz, we had 2 very clear frequencies in our spectrum. However, at 11Hz, the second peak disappeared entirely! We can think of it as \"hiding\" in the 1Hz peak in the spectrum.\n",
    "\n",
    "The **Nyquist Theorem** tells us how fast we need to sample in order to prevent aliasing. It states that in order to avoid aliasing, our sampling frequency $f_s$ must be at least twice the highest frequency present in the signal ($f_s > 2 * f_{max}$). In practice, due to noise, there is no maximum frequency of the signal, so we always have some aliasing. This can be minimized by using an analog anti-aliasing filter before we sample. Note that the Nyquist theorem holds in discrete time as well. Namely, if we want to downsample a recording, then the most we can sample is by a factor of $M$ (i.e take every Mth sample) such that $\\frac{\\pi}{M} > 2 * \\omega_{max}$.\n",
    "\n",
    "### Exercise\n",
    "How much can we downsample our audio clip before aliasing starts to degrade our audio quality? Which parts of the audio degrade first (hint, think about which frequencies are masked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_downsampled = audio[::2]\n",
    "ipd.Audio(two_downsampled, rate=fs // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_downsampled = audio[::4]\n",
    "ipd.Audio(four_downsampled, rate=fs // 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_downsampled = audio[::8]\n",
    "ipd.Audio(eight_downsampled, rate=fs // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixteen_downsampled = audio[::16]\n",
    "ipd.Audio(sixteen_downsampled, rate=fs // 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-necessity",
   "metadata": {},
   "source": [
    "## IV.b Quantization\n",
    "\n",
    "Earlier, we allowed our discrete signals to be functions from $\\mathbb{N}\\to\\mathbb{R}$. In words, we discretized time, but our signal took values over a continuous range. This is not entirely accurate since computers require use bits to represent numbers, so if we use $B$ bits to represent the values our signal takes on, we can only represent $2^B$ possible values.\n",
    "\n",
    "### Exercise\n",
    "See how changing the number of bits we use to represent audio impacts the quality of the audio (currently using 16bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio // 4096, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-parks",
   "metadata": {},
   "source": [
    "# V. Spectral Analysis - Hoang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-voice",
   "metadata": {},
   "source": [
    "## V.a Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87212bd6",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "* We can only capture a finite length of a signal\n",
    "* Impossible to capture an infinitely long signal (x[n] from $n = -\\infty$ to $n = \\infty$\n",
    "\n",
    "**How?**\n",
    "* Time domain: Multiple the signal x[n] with a window: $x[n] \\cdot w[n]$\n",
    "* Frequency domain: Convolution between the spectrum and the DTFT of the window, thus blurring the spectrum\n",
    "\n",
    "**Popular Windows**\n",
    "* Rectangular (never use this due to excessive sidelobe leakage)\n",
    "* Hann\n",
    "* Hamming\n",
    "* Tukey\n",
    "* Blackman\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-exhaust",
   "metadata": {},
   "source": [
    "## V.b STFT/Spectrogram\n",
    "\n",
    "**Overview**\n",
    "\n",
    "<img src=\"./images/STFT_steps.png\" alt=\"Step-by-step to perform STFT\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c961a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio signal attributes\n",
    "N = audio.size\n",
    "Tmax = N/fs\n",
    "print(f\"Sampling rate: {fs} Hz\")\n",
    "print(f\"Number of samples: {N} samples\")\n",
    "print(f\"Duration of recording: {Tmax} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Parameters\n",
    "Tw_s = 1.0 # window duration (seconds)\n",
    "s_s = 0.01 # step size (seconds)\n",
    "M = 4096 # number of frequencies (number of zeropadding)\n",
    "L = 296 # length of autocorrelation window (samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3809c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = np.arange(N)\n",
    "t = np.linspace(0, Tmax, N)\n",
    "Tw = int(np.round(Tw_s * fs)) # window duration (samples)\n",
    "s = int(np.round(s_s * fs)) # step size (samples)\n",
    "nS = int(np.ceil((N-Tw)/s)) # number of steps\n",
    "\n",
    "print(f\"Data window duration: {Tw} (samples)\")\n",
    "print(f\"Step size: {s} (samples)\")\n",
    "print(f\"Number of steps: {nS} (steps)\")\n",
    "\n",
    "# Divide the signal into segments of length Tw (window duration)\n",
    "audio_segs = []\n",
    "for idx in [int(i) for i in np.arange(0, N-Tw, s)]:\n",
    "    audio_segs.append(audio[idx:idx+Tw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(x, fs, ml):\n",
    "    '''\n",
    "        x (numpy.ndarray): time-domain signal\n",
    "        fs (float): sampling rate (Hz)\n",
    "        ml (int): maximum lag (samples)\n",
    "    '''\n",
    "    nx = len(x) # number of samples in signal\n",
    "    ml = min(ml, nx) # maximum lag\n",
    "    mx = np.mean(x) # mean of signal\n",
    "    vx = np.var(x) # variance of signal\n",
    "    x = x - mx; # remove the mean (DC component) from the signal\n",
    "\n",
    "    nz = int(2**np.ceil(np.log2(np.abs(2*nx-1)))) # number of zeropadding\n",
    "    X  = np.fft.fft(x, nz)\n",
    "    xc = np.fft.ifft(np.abs(X)**2)\n",
    "    ac = np.real(xc[1:ml])\n",
    "    ac = ac/nx # biased form of autocorrelation estimate\n",
    "    ac = ac/ac[0]\n",
    "\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a548a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram\n",
    "k = np.arange(0, M)\n",
    "we = k * (2*np.pi/M) # angular frequency of estimate (rad/s)\n",
    "fe = k * fs/M # frequency of estimate (Hz)\n",
    "wd = np.blackman(Tw) # data window\n",
    "wa = np.blackman(L) # autocorrelation window\n",
    "PSD_segs = []\n",
    "for idx in [int(i) for i in np.arange(0, nS)]:\n",
    "    # Window the signal then compute the autocorrelation\n",
    "    r_x_tmp = autocorrelation(audio_segs[idx] * wd, fs, Tw+1)\n",
    "    r_x = np.concatenate((np.flip(r_x_tmp[1:-1]), r_x_tmp))\n",
    "    \n",
    "    # Window the autocorrelation\n",
    "    if L >= len(r_x):\n",
    "        r_v = r_x * wa[int(abs(L-len(r_x))/2) : int(len(r_x)-1-abs(L-len(r_x))/2)];\n",
    "    else:\n",
    "        # TODO: debug here\n",
    "        r_v = wa * r_x[int(abs(L-len(r_x))/2) : int(len(r_x)-1-abs(L-len(r_x))/2)];\n",
    "\n",
    "    # Compute the PSD\n",
    "    PSD_segs.append(np.exp(-1j * we * 1) * np.fft.fft(r_v, M))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-intermediate",
   "metadata": {},
   "source": [
    "## V.c Spectral Estimation Methods\n",
    " \n",
    "**Periodogram**\n",
    "* Has excessive variance\n",
    "\n",
    "**Blackman-Tukey**\n",
    "* Reduce variance by smoothing the periodogram\n",
    "\n",
    "**Welch-Bartlett**\n",
    "* Reduce variance by averaging the periodogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-samba",
   "metadata": {},
   "source": [
    "## V.c Tradeoffs when performing spectral analysis\n",
    "\n",
    "**Overview**\n",
    "\n",
    "|      Spectrogram Design Parameters     |                              Tradeoffs                             |\n",
    "|:--------------------------------------:|:------------------------------------------------------------------:|\n",
    "|               Window types             |      Frequency resolution (mainlobe width) vs. Sidelobe leakage    |\n",
    "|           Data window duration         |     Frequency resolution vs. Time resolution; Bias vs. Variance    |\n",
    "|     Autocorrelation window duration    |                          Bias vs. Variance                         |\n",
    "|            Step size/Overlap           |                   Computation power vs. Resolution                 |\n",
    "\n",
    "**Data window duration tradeoff**\n",
    "\n",
    "|     Data Window Duration    |     Frequency Resolution    |     Time Resolution    |     PSD Bias    |     PSD Variance    |\n",
    "|:---------------------------:|:---------------------------:|:----------------------:|:---------------:|:-------------------:|\n",
    "|             Long            |             High            |           Low          |        Low      |         High        |\n",
    "|             Short           |              Low            |           High         |       High      |          Low        |\n",
    "\n",
    "**Autocorrelation window duration tradeoff**\n",
    "\n",
    "<img src=\"./images/autocorrelation_window_duration_tradeoff.png\" alt=\"Autocorrelation window duration tradeoff\" width=\"250\"/>\n",
    "\n",
    "**Step size/Overlap tradeoff**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-threshold",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. [Anmol's Course Notes from EE120 (Signals and Systems)](https://aparande.gitbook.io/berkeley-notes/ee120-0)\n",
    "2. [Anmol's Course Notes from EE123 (Digital Signal Processing)](https://aparande.gitbook.io/berkeley-notes/ee123-0)\n",
    "3. [Discrete Time Signal Formula Sheet](https://anmolparande.com/resources/berkeley/discrete-formula-sheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-italic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
