{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranking-castle",
   "metadata": {},
   "source": [
    "# Basics of Signal Processing\n",
    "**Authors**: Anmol Parande, Hoang Nguyen, Jordan Grelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import scipy.signal as signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-brass",
   "metadata": {},
   "source": [
    "Throughout this notebook, we will be working with a clip from [Suzanne Vega's song, \"Tom's Diner\"](https://www.youtube.com/watch?v=FLP6QluMlrg). We will use `scipy` to read the audio data from the `.wav` file. It will return the sampling frequency `fs` as well as the audio samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, audio = wavfile.read(\"toms-diner.wav\")\n",
    "print(f\"Loaded {audio.size} samples at a sampling rate of {fs}Hz\")\n",
    "ipd.Audio(audio, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-bacteria",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "I. Time Domain Filtering\n",
    "\n",
    "II. DFT\n",
    "\n",
    "III. Frequency Domain Filtering\n",
    "\n",
    "IV. Sampling Theory\n",
    "\n",
    "V. Spectral Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-lease",
   "metadata": {},
   "source": [
    "# I. Time Domain Filtering\n",
    "\n",
    "A discrete signal can be thought of as a function mapping integers to real values (i.e a function $f: \\mathbb{N}\\to\\mathbb{R})$). This is the so-called \"time-domain\" representation of the signal because the integers often represent time in some sense.\n",
    "\n",
    "A system is a process which takes a signal as an input and returns a signal as an output. Digital systems which we use to process signals are called filters. Systems can have several different properties. Two important ones are linearity and time-invariance.\n",
    "\n",
    "**Linearity**: A system $H$ is linear if given input signal $x$, input signal $y$, and scalars $\\alpha$ and $\\beta$, $H(\\alpha x + \\beta y) = \\alpha H(x) + \\beta H(y)$\n",
    "\n",
    "**Time-Invariance**: A system is time-invariant when shifting the input signal in time results in an equal shift in time for the output signal (i.e if $H$ transforms $x[n]$ into $y[n]$, then $H$ transforms $x[n-N]$ into $y[n-N]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-professional",
   "metadata": {},
   "source": [
    "## I.a Linear Filtering\n",
    "\n",
    "When a system is Linear and Time-Invariant, we can characterize systems by their impulse response. The impulse response of a system $H$ is given by $h[n] = H(\\delta[n])$ where $$\\delta[n] = \\begin{cases} 1 & \\text{ if, } n=0\\\\ 0 & \\text{ else.} \\end{cases}$$\n",
    "\n",
    "This is useful because it means we can compute the response of the system by doing a **convolution** of the input with the impulse response.\n",
    "\n",
    "$$(x * y)[n] = \\sum_{k=-\\infty}^{\\infty}x[k]y[n-k] $$\n",
    "\n",
    "For example, we can take a moving average by using the filter\n",
    "$$ h_{avg}[n] = \\begin{cases} \\frac{1}{5} & \\text{ if } 0 \\leq n \\leq 4\\\\ 0 & \\text{ else.}\\end{cases} $$\n",
    "\n",
    "We can also define a so-called \"edge detector\" filter in order to detect edges in the audio.\n",
    "$$ h_{edge}[n] = \\begin{cases} (-1)^n & \\text{ if } 0 \\leq n \\leq 1\\\\ 0 & \\text{ else.}\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge detector and moving average filters\n",
    "\n",
    "plt.stem(np.linspace(0, 100/44.1, 100), audio[:100])\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()\n",
    "\n",
    "hi_pass = np.array([(-1)**n for n in range(2)])/2.0\n",
    "lo_pass = np.array([1 for n in range(5)])/5.0\n",
    "\n",
    "plt.stem(hi_pass)\n",
    "plt.xlabel(\"samples\")\n",
    "plt.show()\n",
    "\n",
    "plt.stem(lo_pass)\n",
    "plt.xlabel(\"samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(np.linspace(0, 100/44.1, 100), np.convolve(audio[:100], hi_pass, \"same\"))\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()\n",
    "\n",
    "plt.stem(np.linspace(0, 100/44.1, 100), np.convolve(audio[:100], lo_pass, \"same\"))\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_pass_song = np.convolve(audio, hi_pass)\n",
    "ipd.Audio(hi_pass_song, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_pass_song = np.convolve(audio, lo_pass)\n",
    "ipd.Audio(hi_pass_song, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-tactics",
   "metadata": {},
   "source": [
    "## I.b Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b19be",
   "metadata": {},
   "source": [
    "Cross-correlation, in signal processing terms, is the process of convolving one signal with a flipped version of another. Cross-correlation produces a graph of correlation versus time, where correlation is the dot-product of the two signals at that particular point.\n",
    "- If you are looking for the timestamps of a particular noise within a longer signal with other sounds present, you may want to cross-correlate the two.\n",
    "- Cross-correlation is used in sonar to detect when the initial pulse (a known signal) is returned.\n",
    "\n",
    "Autocorrelation is the practice of cross-correlating a signal with itself. It is helpful for eliminating noise, as true frequencies will be preserved due to being periodic, while noise tends to be reduced.\n",
    "\n",
    "$$ r_x(n) = x[n] * x[-n] = \\sum_{k=-\\infty}^{\\infty} x[k] x[n-k] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change this value and see how the noise amplitude affects the signal before and after autocorrelation!\n",
    "noise_amplitude = 2 \n",
    "\n",
    "sample = np.sin(2 * np.pi * np.arange(50)/16)\n",
    "noise = 2*np.random.random(50) - 1\n",
    "noisy_sample = sample+noise\n",
    "autocorr = np.convolve(noisy_sample, np.flip(noisy_sample))\n",
    "plt.stem(np.linspace(0,49/44.1, 50), noisy_sample)\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()\n",
    "plt.stem(np.linspace(-49/44.1,50/44.1, 99), autocorr)\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()\n",
    "# ipd.Audio(audio, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ed8b3",
   "metadata": {},
   "source": [
    "Autocorrelation will always have a peak in the middle, which will grow larger relative to the rest of the signal the more noisy your signal is. This peak has a strength equal to the overall power of the signal, since it occurs at an offset of zero (meaning the signal is completely overlapping with itself, and the magnitude is $\\sum^{N}_{n=1} X[n]^2$. \n",
    "\n",
    "**Comprehension Question:**\n",
    "\n",
    "However, notice even when the signal is highly corrupted, you can still make out the base frequency in the autocorrelated signal. Why is this?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Random noise tends to cancel when there is any offset, but pure frequencies still make it through. This is related to how convolution in the time domain equals multiplication in the frequency domain, and so pure frequencies will stand out above the noise when they are squared. We'll go over more about this later. Thus autocorrelation is often used to denoise signals. \n",
    "\n",
    "**Comprehension question:**\n",
    "\n",
    "Why is the signal contained within a triangular envelope?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The length of the autocorrelation goes from -N to N, where N is the number of samples in the original signal. When some points are not overlapping, the non-overlapping points cannot contribute to the signal at all. The window this creates is effectively equivalent to convolving 2 boxes, which makes a triangular envelope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-flesh",
   "metadata": {},
   "source": [
    "## I.c Nonlinear Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8e32a",
   "metadata": {},
   "source": [
    "Sometimes you end up with a signal that has salt and pepper noise (random bits set to 0 or 1) due to corruption or problems with a sensor. Nonlinear filtering, such as median filtering, applies a non-linear filter so that extremely high peaks made by these errors can be filtered out without disproportionately affecting the surrounding signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18e91e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salt_and_pepper = np.random.binomial(1, 0.01, size=audio.shape) * 32000 + np.random.binomial(1, 0.01, size=audio.shape) * -32000\n",
    "audio_corrupted = audio+salt_and_pepper\n",
    "plt.stem(np.linspace(0,999/44.1, 1000), audio_corrupted[:1000])\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_corrupted, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145699e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# median filter docs: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html\n",
    "# try changing the next cell to see how different mean and median filtering sound!\n",
    "audio_medfilt = signal.medfilt(audio_corrupted, kernel_size=5)\n",
    "\n",
    "meanfilt = np.array([.2, .2, .2, .2, .2])\n",
    "audio_meanfilt = np.convolve(audio_corrupted, meanfilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae295a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_medfilt, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-rental",
   "metadata": {},
   "source": [
    "# II. DFT\n",
    "\n",
    "Typically, when we look at signals, we look at them in the so-called time-domain. Each sample $x[k]$ represents the amplitude of the signal at time-step $k$. This tells us what the signal looks like. One question we might want to ask ourselves is _\"How fast is the signal changing?\"_\n",
    "\n",
    "For sinusoidal signals like $x[n] = \\cos(\\omega n)$ and $x[n] = \\sin(\\omega n)$, answering this question is easy because a larger $\\omega$ means the signal is changing faster ($\\omega$ is known as the angular frequency). For example, consider the plots below which each consist of 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 100, 100)\n",
    "slow_cos = np.cos(2 * np.pi * n / 100)\n",
    "fast_cos = np.cos(2 * np.pi * 5 * n / 100)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.stem(n, slow_cos, use_line_collection=True)\n",
    "plt.title(\"$\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)$\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"$\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)$\")\n",
    "plt.stem(n, fast_cos, use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-paper",
   "metadata": {},
   "source": [
    "$\\cos\\left(\\frac{10\\pi}{100} t\\right)$ is clearly changing a lot faster. If we allow ourselves to consider complex signals, then we can generalized sinusoids using the complex exponential $e^{j\\omega}$. Just like real sinusoids, the $\\omega$ in the signal $x[n] = e^{j\\omega n}$ determines how fast the signal changes (i.e rotates around the unit circle). If we can somehow \"project\" our time-domain signal $x[n]$ onto a \"basis\" of complex exponential signals, then, then the coefficients $X[k]$ should tell us how much the signal changes.\n",
    "\n",
    "The Discrete Fourier Transform is the change of basis which we use for a finite, length-$N$ signal to understand how fast it is changing. The basis used in the DFT are the $N$th roots of unity (i.e the complex solutions to $\\omega=1$). More specifically, the $k$th basis vector is given by $\\phi_k[n] = e^{j\\frac{2\\pi}{N}kn}$. Using the complex inner product $\\langle \\vec{x}, \\vec{y} \\rangle = \\vec{y}^*\\vec{x}$, the DFT coefficients are given by\n",
    "\n",
    "$$X[k] = \\langle x, \\phi_k \\rangle = \\sum_{n=0}^{N-1}x[n]e^{-j\\frac{2\\pi}{N}kn}.$$\n",
    "\n",
    "From the DFT coefficients, we can recover the time-domain coefficients using the inverse DFT.\n",
    "\n",
    "$$x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1}X[k]e^{j\\frac{2\\pi}{N}kn}.$$\n",
    "\n",
    "There are many ways to compute the DFT. The fastest method is the Fast Fourier Transform (FFT), which is an algorithm which computes the DFT. It is built into `numpy` as part of the `fft` submodule.\n",
    "\n",
    "If we look at the DFT coefficients of the two cosines we saw earlier, we can see that it is indeed doing exactly what we wanted it to: characterizing the frequency of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_cos_fft = np.fft.fft(slow_cos)\n",
    "fast_cos_fft = np.fft.fft(fast_cos)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.stem(n, np.abs(slow_cos_fft), use_line_collection=True)\n",
    "plt.title(\"$|DFT\\{\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)\\}|$\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"$|DFT\\{\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)\\}|$\")\n",
    "plt.stem(n, np.abs(fast_cos_fft), use_line_collection=True)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.stem(n, np.angle(slow_cos_fft), use_line_collection=True)\n",
    "plt.title(\"$\\\\arg \\\\left(DFT\\{\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)\\}\\\\right)$\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"$\\\\arg \\\\left(DFT\\{\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)\\}\\\\right)$\")\n",
    "plt.stem(n, np.angle(fast_cos_fft), use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-soundtrack",
   "metadata": {},
   "source": [
    "Since $\\cos\\left(\\frac{2\\pi}{100}n\\right) = \\frac{1}{2}\\left(e^{j\\frac{2\\pi}{100}n} + e^{-j\\frac{2\\pi}{100}n}\\right)$, we should expect peaks at $k = 1$ and $k =-1$ (note that because the roots of unity are periodic, $k=-1$ is the same basis vector as $k=99$). Likewise, since $\\cos\\left(\\frac{10\\pi}{100}n\\right) = \\frac{1}{2}\\left(e^{j\\frac{10\\pi}{100}n} + e^{-j\\frac{10\\pi}{100}n}\\right)$, we should expect peaks at $k=5$ and $k=-5$.\n",
    "\n",
    "There are a few things to note:\n",
    "1. The DFT coefficients are complex numbers, so we need both magnitude (top plots) and phase (bottom plots) to characterize the signal information\n",
    "2. For both $\\cos\\left(\\frac{2\\pi}{100}n\\right)$ and $\\cos\\left(\\frac{10\\pi}{100}n\\right)$, we should only expect 2 non-zero coefficients. However, we have apparently many non-zero coefficients. These are due to numerical instability in the FFT algorithm (if you print them out, these coefficients are on the order of $10^{-3}$ in magnitude and so are insignificant).\n",
    "3. The DFT basis is **not** orthonormal. This is why we must scale by $\\frac{1}{N}$ when applying the inverse DFT (`np.fft.ifft` in numpy). This is also why the peak magnitudes of the example signals above are 50 and not $\\frac{1}{2}$.\n",
    "4. DFT basis vectors are complex conjugates of each other (i.e $\\phi_k[n] = \\phi_{N-k}[n]^*$). This means for real signals, $X[k] = X^*[N-k]$.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-portsmouth",
   "metadata": {},
   "source": [
    "To get a better feel for the DFT, compute and plot the magnitude of the DFT coefficients of our clip from Tom's Diner in decibels ($dB = 20\\log_{10}(\\cdot)$). Since our song is a real signal, do not plot the complex conjugate coefficients since they are redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-memorabilia",
   "metadata": {},
   "source": [
    "**Comprehension Question**: Do you notice anything interesting about the chart above?\n",
    "\n",
    "**Answer**: Around index 150,000, there is a sharp decline in the magnitude of the DFT coefficients. It turns out that this DFT coefficient represents approximately 12.5 kHz (we'll see how to compute this later), which is close to the human hearing limit of about 20kHz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-canadian",
   "metadata": {},
   "source": [
    "**Comprehension Question**: What does the first coefficient $X[0]$ of the DFT represent in simple terms?\n",
    "\n",
    "**Answer**: It is the sum of the signal (we can see this from the formula by letting $k=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-salem",
   "metadata": {},
   "source": [
    "## II.a PSD\n",
    "\n",
    "In signal processing, due to noise, numerical stability, and other issues, we often care about the dominant frequencies in the signal (e.g when we are looking for formants in a vowel). This means we want to look at the magnitude of the DFT coefficients. However, sometimes peaks in the DFT are difficult to distinguish when looking at a magnitude plot. To better distinguish peaks, we can instead look at $|X[k]|^2$, the so-called **Power Spectral Density (PSD)**.\n",
    "\n",
    "The Power Spectral Density is the essentially the magnitude of the DFT of the auto-correlation of the signal $x$. This is because when $x[n]$ has DFT coefficients $X[k]$, then $x[-n]$ has DFT coefficients $X^*[k]$ and since auto-correlation is the convolution of $x[n] * x[-n]$, and convolution in the time-domain is multiplication in the frequency domain, $PSD = X[k] X^*[k] = |X[k]|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-adaptation",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Remember that formants are the dominant frequencies in vowels. That means we can use the PSD to roughly find formants and distinguish vowels from each other.\n",
    "\n",
    "We have two mystery recordings taken from [this source](https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/). They were sampled at 16000Hz. Try and distingiush them by their dominant frequencies, we will go through the following procedure.\n",
    "\n",
    "1. Split the recording into 25ms sections\n",
    "2. Find the PSD of each section\n",
    "3. Let the \"PSD\" of the recording be the mean value of the PSD of each section at each particular point. This will help pick out the frequencies that are dominant in any section of the recording\n",
    "4. Try and guess where the formants are. Can you tell by their relative positions which vowel each recording is?\n",
    "\n",
    "When plotting, you can use the `FREQS` variable for the x-axis of the plot, and remember, the complex conjugate coefficients of the DFT give you no extra information, so do not plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQS = np.linspace(0, 8000, 200)\n",
    "_, vowel_1 = wavfile.read(\"mystery_vowel_1.wav\")\n",
    "_, vowel_2 = wavfile.read(\"mystery_vowel_2.wav\")\n",
    "\n",
    "# Cut each recording to an appropriate length\n",
    "vowel_1 = vowel_1[13:]\n",
    "vowel_2 = vowel_2[114:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "\n",
    "plt.title(\"PSD\")\n",
    "plt.plot(FREQS, vowel_1_psd)\n",
    "plt.plot(FREQS, vowel_2_psd)\n",
    "plt.legend([\"Vowel 1\", \"Vowel 2\"])\n",
    "plt.xlabel(\"Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-shock",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-third",
   "metadata": {},
   "source": [
    "Listen to the audio and see if you were right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(vowel_1, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(vowel_2, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-filter",
   "metadata": {},
   "source": [
    "# III. Frequency Domain Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e609181",
   "metadata": {},
   "source": [
    "One really nice property of the DFT is that convolution in the time domain is equivalent to multiplication in the frequency domain, and convolution in frequency is equivalent to multiplication in time.\n",
    "- Important implications to sampling theory, which will be covered in the next section\n",
    "- Makes convolution much more efficient: convolution in time on long signals of length n is $O(n^2)$, while converting to frequency domain, multiplying, and converting back is $O(n \\log{n})$\n",
    "- Makes it easy to control what frequencies you filter. If you want a high-pass filter with a specific cutoff for example, or a band-pass filter to only capture F0, you can design the filter in the frequency domain and then convert it back to the time domain!\n",
    "\n",
    "\n",
    "#### The Frequency Response\n",
    "\n",
    "Just like any signal in the time domain can be transformed into the frequency domain, so can every filter. For every impulse response $h[n]$ in the time domain, we can calculate $H[k]$ in the frequency domain by performing the DFT. Since multiplication in the frequency domain is equivalent to convolution in time, we can actually uniquely characterize any LTI filter $H$ by its impulse response or frequency response. Oftentimes, it is easier to design filters in the frequency domain. For example, in speech recognition where the signal is easily separable by formant in the frequency domain, and we want to design a filter that captures speech while ignoring other frequencies. We don't much care what the shape of the filter is in the time domain, so we can design a filter in the frequency domain and impose restrictions on it to create the filter we need.\n",
    "\n",
    "#### Types of Filters\n",
    "\n",
    "Highpass filters let high frequencies through, while blocking low frequencies. This kind of filter is used to filter out low frequency background interference from power lines or machinery.\n",
    "\n",
    "Lowpass filters let low frequencies through, while blocking high frequencies. This kind of filter is often used to reduce high frequency noise, or eliminate aliasing from downsampling.\n",
    "\n",
    "Bandpass/bandstop filters pass or block only a specific range of frequencies. This might be useful in audio processing for focusing solely on F1 or F2, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbac431",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fft = np.fft.fft(audio)\n",
    "len_clip = audio.shape[0]\n",
    "\n",
    "# You can try adjusting the cutoff frequency or changing the list comprehension\n",
    "# under freq_filter to create different frequency responses!\n",
    "cutoff_frequency = 500 * len_clip/fs\n",
    "freq_filter = np.array([1 if n < cutoff_frequency else 0 for n in range(len_clip)])\n",
    "time_filter = np.real(np.fft.ifft(freq_filter))\n",
    "audio_fft = np.fft.fft(audio)\n",
    "audio_filtered = np.real(np.fft.ifft(audio_fft*freq_filter))\n",
    "\n",
    "plt.plot(np.linspace(0, 999/44.1, 1000), audio[:1000])\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(freq_filter[:44100], color='r')\n",
    "plt.xlabel(\"frequency (Hz)**\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.linspace(0, 999/44.1, 1000),audio_filtered[:1000])\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4edb5ba",
   "metadata": {},
   "source": [
    "We can also design filters in the frequency domain to optimize for specific conditions, rather than simply using a box filter in the frequency domain (which has drawbacks, as we'll see later). One example of such a filter is the Butterworth Filter, which is designed to minimize variation in the frequency response in the passband, and thus avoid distortions in the output signal.\n",
    "\n",
    "The Butterworth filter has 3 main parameters:\n",
    "\n",
    "- **N**, the order of the filter: how sharp the cutoff is\n",
    "\n",
    "- $\\boldsymbol{\\omega_h}$, the cutoff frequency: the frequency at which the frequency response drops to $1/\\sqrt{2}$ of the passband response\n",
    "\n",
    "- **btype**, the type of filter (ie, lowpass, highpass, bandpass, bandstop)\n",
    "\n",
    "Documentation can be found at: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html#scipy.signal.butter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Try adjusting the parameters of the Butterworth filter to see how it affects \n",
    "# the frequency response and sound!\n",
    "\n",
    "sos = signal.butter(2, 500, 'hp', fs=fs, output='sos')\n",
    "audio_filtered = signal.sosfilt(sos, audio)\n",
    "b, a = signal.butter(2, 500, 'low', analog=True)\n",
    "w, h = signal.freqs(b, a)\n",
    "plt.semilogx(w, 20 * np.log10(abs(h)), color='r')\n",
    "plt.title('Butterworth filter frequency response')\n",
    "plt.xlabel('Frequency [radians / second]')\n",
    "plt.ylabel('Amplitude [dB]')\n",
    "plt.margins(0, 0.1)\n",
    "plt.grid(which='both', axis='both')\n",
    "plt.axvline(500, color='green') # cutoff frequency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Listen to the song filtered with different Butter and box filters\n",
    "\n",
    "ipd.Audio(audio_filtered, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-andrew",
   "metadata": {},
   "source": [
    "# IV. Sampling Theory\n",
    "\n",
    "In the real-world, most signals are continuous (i.e they are functions from $\\mathbb{R}\\to\\mathbb{R}$). Meanwhile, computers operate in the discrete space (i.e they are functions from $\\mathbb{N}\\to\\mathbb{R}$. This means that in order to analyze any continuous signal, we need to somehow discretize it so it can be stored in finite memory.\n",
    "\n",
    "Given a continuous signal $x_c(t)$, we can obtain a discrete signal by letting $x_d[n] = x_c(f(n))$ where $f: \\mathbb{N}\\to\\mathbb{R}$ describes our sampling scheme.\n",
    "\n",
    "A **uniform, non-adaptive sampling** scheme is where we pick some sampling frequency $\\omega_s$ and let $f(n) = \\frac{n}{\\omega_s}$. We can think of it as \"saving\" the value of the continuous time signal every $\\frac{1}{\\omega_s}$ seconds. _Uniform_ means that $\\omega_s$ is constant (i.e it does not depend on $n$), and _non-adaptive_ means $\\omega_s$ is independent of the samples we have seen so far. Uniform, non-adaptive sampling schemes are what we most frequently use for sampling because of their simplicity and well-known theoeretical guarantees. For the rest of the notebook, we will assume all sampling is uniform and non-adaptive.\n",
    "\n",
    "Because sampling has the potential to destroy information, we need to understand how it impacts the frequency domain. In continuous time, frequencies exist on the range $[0, \\infty)$. However, in discrete time, the fastest that a signal can change is $\\pi$ radians / sample (i.e alternating from 1 to -1 like $\\cos(\\pi n)$). When we take the DFT of a signal that we sampled, we want to know how our angular frequencies relate to the continuous frequencies.\n",
    "\n",
    "The easiest way to think of how continuous frequencies relate to discrete frequencies is by mapping the interval $\\left[0, \\frac{f_s}{2}\\right]$ (continuous frequencies) to the interval $[0, \\pi]$ (angular frequencies). Given an angular frequency $\\omega_d\\in[0, \\pi]$, the continuous frequency that it represent $\\omega_c = \\frac{f_s}{2\\pi}\\omega_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-latitude",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Plot the magnitude of DFT coefficients (in decibels) of our clip from Tom's Diner and label the x-axis with the continuous time frequencies. Ignore the complex conjugate coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "\n",
    "plt.xlabel(\"Hz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-mambo",
   "metadata": {},
   "source": [
    "## IV.a Aliasing\n",
    "\n",
    "How frequently we sample matters a lot. If we sample too slowly, then we lose information. If we sample too fast, then we are wasting memory. The three plots below are all samples of a 10 second long sine wave $x(t) = \\sin(2\\pi t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "hundred_hz = np.linspace(0, 10, 1000)\n",
    "ten_hz = np.linspace(0, 10, 100)\n",
    "one_hz = np.linspace(0, 10, 10)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(one_hz, np.sin(2 * np.pi * one_hz))\n",
    "plt.title(\"$f_s$ = 1Hz\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ten_hz, np.sin(2 * np.pi * ten_hz))\n",
    "plt.title(\"$f_s$ = 10Hz\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(hundred_hz, np.sin(2 * np.pi * hundred_hz))\n",
    "plt.title(\"$f_s$ = 100Hz\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-platform",
   "metadata": {},
   "source": [
    "Notice how the faster sampling frequencies 10Hz and 100Hz look virtually identical and cycle 10 times in 10 seconds as we expect a 1Hz sine wave to do. However, when we sample at 1Hz, our samples look like they came from a 0.1Hz sine wave, not a 1Hz sine wave. When higher frequencies \"masquerade\" as lower frequencies, this is known as **aliasing**. The effects of aliasing are very clear in the frequency domain through the following example where we sample the signal $x_c(t) = \\sin(2\\pi t) + \\sin(2\\pi * 10t)$ with a sampling frequency of 11Hz vs a sampling frequency of 50Hz vs a sampling frequency of 1000Hz over the course of 1 second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-riding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def x_c(t):\n",
    "    return np.sin(2 * np.pi * t) + np.sin(2 * np.pi * 10 * t)\n",
    "\n",
    "eleven_hz = np.linspace(0, 1, 11)\n",
    "fifty_hz = np.linspace(0, 1, 50)\n",
    "thousand_hz = np.linspace(0, 1, 1000)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(eleven_hz, x_c(eleven_hz))\n",
    "plt.title(\"$f_s$ = 11Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(fifty_hz, x_c(fifty_hz))\n",
    "plt.title(\"$f_s$ = 50Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(thousand_hz, x_c(thousand_hz))\n",
    "plt.title(\"$f_s$ = 1000Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(np.linspace(0, 11, eleven_hz.size), np.abs(np.fft.fft(x_c(eleven_hz))))\n",
    "plt.title(\"$f_s$ = 11Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(np.linspace(0, 50, fifty_hz.size), np.abs(np.fft.fft(x_c(fifty_hz))))\n",
    "plt.title(\"$f_s$ = 50Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(np.linspace(0, 1000, thousand_hz.size), np.abs(np.fft.fft(x_c(thousand_hz))))\n",
    "plt.title(\"$f_s$ = 1000Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-inside",
   "metadata": {},
   "source": [
    "When we sampled at 50Hz, we had 2 very clear frequencies in our spectrum. However, at 11Hz, the second peak disappeared entirely! We can think of it as \"hiding\" in the 1Hz peak in the spectrum. At 1000Hz, we can measure a much larger range of frequencies, and so all of our peaks remain in the plot (they look squished together due to the scale of the axis).\n",
    "\n",
    "The **Nyquist Theorem** tells us how fast we need to sample in order to prevent aliasing. It states that in order to avoid aliasing, our sampling frequency $f_s$ must be at least twice the highest frequency present in the signal ($f_s > 2 * f_{max}$). In practice, due to noise, there is no maximum frequency of the signal, so we always have some aliasing. This can be minimized by using an analog anti-aliasing filter before we sample. Note that the Nyquist theorem holds in discrete time as well. Namely, if we want to downsample a recording, then the most we can sample is by a factor of $M$ (i.e take every Mth sample) such that $\\frac{\\pi}{M} > 2 * \\omega_{max}$.\n",
    "\n",
    "### Exercise\n",
    "How much can we downsample our audio clip before aliasing starts to degrade our audio quality? Which parts of the audio degrade first (hint, think about which frequencies are masked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-necessity",
   "metadata": {},
   "source": [
    "## IV.b Quantization\n",
    "\n",
    "Earlier, we allowed our discrete signals to be functions from $\\mathbb{N}\\to\\mathbb{R}$. In words, we discretized time, but our signal took values over a continuous range. This is not entirely accurate since computers require use bits to represent numbers, so if we use $B$ bits to represent the values our signal takes on, we can only represent $2^B$ possible values.\n",
    "\n",
    "### Exercise\n",
    "See how changing the number of bits we use to represent audio impacts the quality of the audio (currently using 16bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c5a76",
   "metadata": {},
   "source": [
    "# V. Spectral Analysis - Hoang\n",
    "\n",
    "## V.a Windowing\n",
    "\n",
    "**Why?**\n",
    "* We can only capture a finite length of a signal\n",
    "* Impossible to capture an infinitely long signal (x[n] from $n = -\\infty$ to $n = \\infty$\n",
    "\n",
    "**How?**\n",
    "* Time domain: Multiple the signal x[n] with a window: $x[n] \\cdot w[n]$\n",
    "* Frequency domain: Convolution between the spectrum and the DTFT of the window, thus blurring the spectrum\n",
    "\n",
    "$$x[n] w[n] \\stackrel{\\mathcal{FT}}{\\Leftrightarrow} \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} X(e^{ju}) W(e^{j(\\omega-u)}) \\,du$$\n",
    "\n",
    "**Important note**\n",
    "* Rectangular window has the best resolution but the most leakage \n",
    "    * Never use this due to excessive sidelobe leakage outweight the resolution gain\n",
    "* Popular window: Hann, Hamming, Tukey, Blackman, etc.\n",
    "    * Not applying one of these windows == rectangular window by default\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f2/Window_functions_in_the_frequency_domain.png\" alt=\"Spectrum of different window functions\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4d5ff",
   "metadata": {},
   "source": [
    "# V.b Spectral Estimation Methods\n",
    " \n",
    "**Periodogram**\n",
    "* Has excessive variance\n",
    "\n",
    "<img src=\"./Images/periodogram.png\" alt=\"Natural spectral estimator: periodogram\" width=\"500\"/>\n",
    "\n",
    "**Blackman-Tukey**\n",
    "* Reduce variance by smoothing the periodogram\n",
    "* Window the autocorrelation before taking the PSD\n",
    "\n",
    "<img src=\"./Images/blackman_tukey.png\" alt=\"Blackman-Tukey estimator\" width=\"500\"/> \n",
    "\n",
    "* Important tradeoff between PSD bias and variance. Control using autocorrelation window duration.\n",
    "\n",
    "<img src=\"./Images/autocorrelation_window_duration_tradeoff.png\" alt=\"Autocorrelation window duration tradeoff\" width=\"200\"/>\n",
    "\n",
    "**Welch-Bartlett**\n",
    "* Reduce variance by averaging the periodogram\n",
    "\n",
    "<img src=\"./Images/welch_bartlett.png\" alt=\"Welch-Bartlett estimator\" width=\"525\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f8df3",
   "metadata": {},
   "source": [
    "## V.c STFT/Spectrogram\n",
    "\n",
    "**Overview**\n",
    "\n",
    "<img src=\"./Images/STFT_steps.png\" alt=\"Step-by-step to perform STFT\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process signal\n",
    "clipped_audio = audio[:len(audio)//2]   # clip the audio in half for better visibility\n",
    "N = clipped_audio.size                  # number of samples in the clipped audio\n",
    "Tmax = N/fs                             # total duration (seconds) of the clipped audio\n",
    "\n",
    "# Print out signal attributes\n",
    "print(f\"Sampling rate: {fs} Hz\")\n",
    "print(f\"Number of samples: {N} samples\")\n",
    "print(f\"Duration of recording: {Tmax} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the spectrogram\n",
    "def plot_spectrogram(t, f, spt):\n",
    "    '''\n",
    "        t (numpy.ndarray): time array\n",
    "        f (numpy.ndarray): frequency array\n",
    "        spt (numpy.ndarray): 2D spectrogram matrix\n",
    "    '''\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.pcolormesh(t, f, np.log10(spt))\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.ylim(0, 15000)\n",
    "    plt.xlabel('Time (sec)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c03e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram parameters\n",
    "Tw_s = 0.01                             # data window duration (seconds)\n",
    "Tw = int(np.round(Tw_s * fs))           # data window duration (samples)\n",
    "no = Tw//1.01                           # number of overlaps\n",
    "\n",
    "print(f'Window duration (seconds): {Tw_s} seconds out of {Tmax} seconds')\n",
    "print(f'Window duration (samples): {Tw} samples out of {clipped_audio.size} samples')\n",
    "print(f'Number of overlaps: Overlapping {no} samples out of {Tw} samples')\n",
    "print('\\n')\n",
    "\n",
    "# Compute the spectrogram, window each segment of the signal by blackman window of length Tw\n",
    "start_time = time.time()\n",
    "f, t, Spectrogram = scipy.signal.spectrogram(x=clipped_audio, fs=fs, window='blackman', nperseg=Tw, noverlap=no, nfft=Tw)\n",
    "print(f\"Spectrogram computation duration: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Plot the spectrogram\n",
    "plot_spectrogram(t=t, f=f, spt=Spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee71b6",
   "metadata": {},
   "source": [
    "## V.d Important Spectral Analysis Tradeoffs\n",
    "\n",
    "|      Spectrogram Design Parameters         |                              Tradeoffs                             |\n",
    "|:--------------------------------------:    |:------------------------------------------------------------------:|\n",
    "|               Window types                 |      Frequency resolution (mainlobe width) vs. Sidelobe leakage    |\n",
    "|           Data window duration             |     Frequency resolution vs. Time resolution; Bias vs. Variance    |\n",
    "|            Step size/Overlap               |                   Computation power vs. Resolution                 |\n",
    "|     Autocorrelation window duration (BT)   |                          Bias vs. Variance                         |\n",
    "\n",
    "**Data window duration tradeoff**\n",
    "* Most important tradeoff\n",
    "\n",
    "|     Data Window Duration    |     Frequency Resolution    |     Time Resolution    |     PSD Bias    |     PSD Variance    |\n",
    "|:---------------------------:|:---------------------------:|:----------------------:|:---------------:|:-------------------:|\n",
    "|             Long            |             High            |           Low          |        Low      |         High        |\n",
    "|             Short           |              Low            |           High         |       High      |          Low        |\n",
    "\n",
    "**Exercise 1: Adjust data window duration to observe Frequency resolution vs. Time resolution tradeoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: Long window --> High frequency resolution, Low time resolution\n",
    "Tw_s = 1                                # data window duration (seconds) # ADJUSTED\n",
    "Tw = int(np.round(Tw_s * fs))           # data window duration (samples)\n",
    "no = Tw//1.01                           # number of overlaps             # FIXED\n",
    "\n",
    "print(f'Window duration (seconds): {Tw_s} seconds out of {Tmax} seconds')\n",
    "print(f'Window duration (samples): {Tw} samples out of {clipped_audio.size} samples')\n",
    "print(f'Number of overlaps: Overlapping {no} samples out of {Tw} samples')\n",
    "print('\\n')\n",
    "\n",
    "# Compute the spectrogram\n",
    "start_time = time.time()\n",
    "f, t, Spectrogram = scipy.signal.spectrogram(x=clipped_audio, fs=fs, window='blackman', nperseg=Tw, noverlap=no, nfft=Tw)\n",
    "print(f\"Spectrogram computation duration: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Plot the spectrogram\n",
    "plot_spectrogram(t=t, f=f, spt=Spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3996d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: Short window --> Low frequency resolution, High time resolution\n",
    "Tw_s = 0.0001                           # data window duration (seconds) # ADJUSTED\n",
    "Tw = int(np.round(Tw_s * fs))           # data window duration (samples)\n",
    "no = Tw//1.01                           # number of overlaps             # FIXED\n",
    "\n",
    "print(f'Window duration (seconds): {Tw_s} seconds out of {Tmax} seconds')\n",
    "print(f'Window duration (samples): {Tw} samples out of {clipped_audio.size} samples')\n",
    "print(f'Number of overlaps: Overlapping {no} samples out of {Tw} samples')\n",
    "print('\\n')\n",
    "\n",
    "# Compute the spectrogram\n",
    "start_time = time.time()\n",
    "f, t, Spectrogram = scipy.signal.spectrogram(x=clipped_audio, fs=fs, window='blackman', nperseg=Tw, noverlap=no, nfft=Tw)\n",
    "print(f\"Spectrogram computation duration: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Plot the spectrogram\n",
    "plot_spectrogram(t=t, f=f, spt=Spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785538b1",
   "metadata": {},
   "source": [
    "**Step size/Overlap tradeoff**\n",
    "* Small step size (more overlaps between segments of the signal) yields better resolution but consumes more computation power\n",
    "\n",
    "**Exercise 2: Adjust number of overlaps for observing Computation power vs. Resolution tradeoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddafeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: 1/2 overlap --> Compute faster but lower resolution\n",
    "Tw_s = 0.01                             # data window duration (seconds) # FIXED\n",
    "Tw = int(np.round(Tw_s * fs))           # data window duration (samples)\n",
    "no = Tw//2                              # number of overlaps             # ADJUSTED\n",
    "\n",
    "print(f'Window duration (seconds): {Tw_s} seconds out of {Tmax} seconds')\n",
    "print(f'Window duration (samples): {Tw} samples out of {clipped_audio.size} samples')\n",
    "print(f'Number of overlaps: Overlapping {no} samples out of {Tw} samples')\n",
    "print('\\n')\n",
    "\n",
    "# Compute the spectrogram\n",
    "start_time = time.time()\n",
    "f, t, Spectrogram = scipy.signal.spectrogram(x=clipped_audio, fs=fs, window='blackman', nperseg=Tw, noverlap=no, nfft=Tw)\n",
    "print(f\"Spectrogram computation duration: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Plot the spectrogram\n",
    "plot_spectrogram(t=t, f=f, spt=Spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60659ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: 0 overlap --> Compute faster but lower resolution\n",
    "Tw_s = 0.01                             # data window duration (seconds) # FIXED\n",
    "Tw = int(np.round(Tw_s * fs))           # data window duration (samples)\n",
    "no = 0                                  # number of overlaps             # ADJUSTED\n",
    "\n",
    "print(f'Window duration (seconds): {Tw_s} seconds out of {Tmax} seconds')\n",
    "print(f'Window duration (samples): {Tw} samples out of {clipped_audio.size} samples')\n",
    "print(f'Number of overlaps: Overlapping {no} samples out of {Tw} samples')\n",
    "print('\\n')\n",
    "\n",
    "# Compute the spectrogram\n",
    "start_time = time.time()\n",
    "f, t, Spectrogram = scipy.signal.spectrogram(x=clipped_audio, fs=fs, window='blackman', nperseg=Tw, noverlap=no, nfft=Tw)\n",
    "print(f\"Spectrogram computation duration: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Plot the spectrogram\n",
    "plot_spectrogram(t=t, f=f, spt=Spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-threshold",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. [Anmol's Course Notes from EE120 (Signals and Systems)](https://aparande.gitbook.io/berkeley-notes/ee120-0)\n",
    "2. [Anmol's Course Notes from EE123 (Digital Signal Processing)](https://aparande.gitbook.io/berkeley-notes/ee123-0)\n",
    "3. [Discrete Time Signal Formula Sheet](https://anmolparande.com/resources/berkeley/discrete-formula-sheet.pdf)\n",
    "4. [EE 525 Course (Statistical Signal Processing) at Portland State University](http://pdx.smartcatalogiq.com/2020-2021/Bulletin/Courses/EE-Electrical-Engineering/500/EE-525)\n",
    "5. [Windowing on Wikipedia](https://en.wikipedia.org/wiki/Window_function)\n",
    "6. [Scipy's Spectrogram Function Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
