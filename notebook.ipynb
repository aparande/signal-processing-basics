{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranking-castle",
   "metadata": {},
   "source": [
    "# Basics of Signal Processing\n",
    "**Authors**: Anmol Parande, Hoang Nguyen, Jordan Grelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-brass",
   "metadata": {},
   "source": [
    "Throughout this notebook, we will be working with a clip from [Suzanne Vega's song, \"Tom's Diner\"](https://www.youtube.com/watch?v=FLP6QluMlrg). We will use `scipy` to read the audio data from the `.wav` file. It will return the sampling frequency `fs` as well as the audio samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, audio = wavfile.read(\"toms-diner.wav\")\n",
    "print(f\"Loaded {audio.size} samples at a sampling rate of {fs}Hz\")\n",
    "ipd.Audio(audio, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-bacteria",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "I. Time Domain Filtering\n",
    "\n",
    "II. DFT\n",
    "\n",
    "III. Frequency Domain Filtering\n",
    "\n",
    "IV. Sampling Theory\n",
    "\n",
    "V. Spectral Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-lease",
   "metadata": {},
   "source": [
    "# I. Time Domain Filtering - Jordan\n",
    "\n",
    "A discrete signal can be thought of as a function mapping integers to real values (i.e a function $f: \\mathbb{N}\\to\\mathbb{R})$). This is the so-called \"time-domain\" representation of the signal because the integers often represent time in some sense.\n",
    "\n",
    "A system is a process which takes a signal as an input and returns a signal as an output. Digital systems which we use to process signals are called filters. Systems can have several different properties. Two important ones are linearity and time-invariance.\n",
    "\n",
    "**Linearity**: A system $H$ is linear if given input signal $x$, input signal $y$, and scalars $\\alpha$ and $\\beta$, $H(\\alpha x + \\beta y) = \\alpha H(x) + \\beta H(y)$\n",
    "\n",
    "**Time-Invariance**: A system is time-invariant when shifting the input signal in time results in an equal shift in time for the output signal (i.e if $H$ transforms $x[n]$ into $y[n]$, then $H$ transforms $x[n-N]$ into $y[n-N]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-professional",
   "metadata": {},
   "source": [
    "## I.a Linear Filtering\n",
    "\n",
    "When a system is Linear and Time-Invariant, we can characterize systems by their impulse response. The impulse response of a system $H$ is given by $h[n] = H(\\delta[n])$ where $$\\delta[n] = \\begin{cases} 1 & \\text{ if, } n=0\\\\ 0 & \\text{ else.} \\end{cases}$$\n",
    "\n",
    "This is useful because it means we can compute the response of the system by doing a **convolution** of the input with the impulse response.\n",
    "\n",
    "$$(x * y)[n] = \\sum_{k=-\\infty}^{\\infty}x[k]y[n-k] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5928461",
   "metadata": {},
   "source": [
    "![convolution.png](convolution.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge detector and moving average filters\n",
    "\n",
    "plt.stem(audio[:100])\n",
    "plt.show()\n",
    "\n",
    "hi_pass = np.array([(-1)**n for n in range(2)])/2.0\n",
    "lo_pass = np.array([1 for n in range(5)])/5.0\n",
    "\n",
    "plt.stem(hi_pass)\n",
    "plt.show()\n",
    "\n",
    "plt.stem(lo_pass)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(np.convolve(audio[:100], hi_pass))\n",
    "plt.show()\n",
    "\n",
    "plt.stem(np.convolve(audio[:100], lo_pass))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_pass_song = np.convolve(audio, hi_pass)\n",
    "ipd.Audio(hi_pass_song, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_pass_song = np.convolve(audio, lo_pass)\n",
    "ipd.Audio(hi_pass_song, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-tactics",
   "metadata": {},
   "source": [
    "## I.b Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b19be",
   "metadata": {},
   "source": [
    "Cross-correlation, in signal processing terms, is the process of convolving one signal with a flipped version of another. Cross-correlation produces a graph of correlation versus time, where correlation is the dot-product of the two signals at that particular point.\n",
    "- If you are looking for the timestamps of a particular noise within a longer signal with other sounds present, you may want to cross-correlate the two.\n",
    "- Cross-correlation is used in sonar to detect when the initial pulse (a known signal) is returned.\n",
    "\n",
    "Autocorrelation is the practice of cross-correlating a signal with itself. It is helpful for eliminating noise, as true frequencies will be preserved due to being periodic, while noise tends to be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.sin(2 * np.pi * np.arange(50)/16)\n",
    "noise = 2*np.random.random(50) - 1\n",
    "noisy_sample = sample+noise\n",
    "autocorr = np.convolve(noisy_sample, np.flip(noisy_sample))\n",
    "plt.stem(noisy_sample)\n",
    "plt.show()\n",
    "plt.stem(np.arange(-49,50), autocorr)\n",
    "plt.show()\n",
    "# ipd.Audio(audio, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ed8b3",
   "metadata": {},
   "source": [
    "Autocorrelation will always have a peak in the middle, which will grow larger relative to the rest of the signal the more noisy your signal is. However, notice even when the signal is highly corrupted, you can still make out the base frequency in the autocorrelated signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-flesh",
   "metadata": {},
   "source": [
    "## I.c Nonlinear Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8e32a",
   "metadata": {},
   "source": [
    "Sometimes you end up with a signal that has salt and pepper noise (random bits set to 0 or 1) due to corruption or problems with a sensor. Nonlinear filtering, such as median filtering, applies a non-linear filter so that extremely high peaks made by these errors can be filtered out without disproportionately affecting the surrounding signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18e91e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salt_and_pepper = np.random.binomial(1, 0.01, size=audio.shape) * 32000 + np.random.binomial(1, 0.01, size=audio.shape) * -32000\n",
    "audio_corrupted = audio+salt_and_pepper\n",
    "plt.stem(audio_corrupted[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_corrupted, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145699e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_medfilt = signal.medfilt(audio_corrupted, kernel_size=5)\n",
    "\n",
    "meanfilt = np.array([.2, .2, .2, .2, .2])\n",
    "audio_meanfilt = np.convolve(audio_corrupted, meanfilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae295a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_medfilt, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-rental",
   "metadata": {},
   "source": [
    "# II. DFT - Anmol\n",
    "\n",
    "Typically, when we look at signals, we look at them in the so-called time-domain. Each sample $x[k]$ represents the amplitude of the signal at time-step $k$. This tells us what the signal looks like. One question we might want to ask ourselves is _\"How fast is the signal changing?\"_\n",
    "\n",
    "For sinusoidal signals like $x[n] = \\cos(\\omega n)$ and $x[n] = \\sin(\\omega n)$, answering this question is easy because a larger $\\omega$ means the signal is changing faster ($\\omega$ is known as the angular frequency). For example, consider the plots below which each consist of 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 100, 100)\n",
    "slow_cos = np.cos(2 * np.pi * n / 100)\n",
    "fast_cos = np.cos(2 * np.pi * 5 * n / 100)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.stem(n, slow_cos, use_line_collection=True)\n",
    "plt.title(\"$\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)$\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"$\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)$\")\n",
    "plt.stem(n, fast_cos, use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-paper",
   "metadata": {},
   "source": [
    "$\\cos\\left(\\frac{10\\pi}{100} t\\right)$ is clearly changing a lot faster. If we allow ourselves to consider complex signals, then we can generalized sinusoids using the complex exponential $e^{j\\omega}$. Just like real sinusoids, the $\\omega$ in the signal $x[n] = e^{j\\omega n}$ determines how fast the signal changes (i.e rotates around the unit circle). If we can somehow \"project\" our time-domain signal $x[n]$ onto a \"basis\" of complex exponential signals, then, then the coefficients $X[k]$ should tell us how much the signal changes.\n",
    "\n",
    "The Discrete Fourier Transform is the change of basis which we use for a finite, length-$N$ signal to understand how fast it is changing. The basis used in the DFT are the $N$th roots of unity (i.e the complex solutions to $\\omega=1$). More specifically, the $k$th basis vector is given by $\\phi_k[n] = e^{j\\frac{2\\pi}{N}kn}$. Using the complex inner product $\\langle \\vec{x}, \\vec{y} \\rangle = \\vec{y}^*\\vec{x}$, the DFT coefficients are given by\n",
    "\n",
    "$$X[k] = \\langle x, \\phi_k \\rangle = \\sum_{n=0}^{N-1}x[n]e^{-j\\frac{2\\pi}{N}kn}.$$\n",
    "\n",
    "From the DFT coefficients, we can recover the time-domain coefficients using the inverse DFT.\n",
    "\n",
    "$$x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1}X[k]e^{j\\frac{2\\pi}{N}kn}.$$\n",
    "\n",
    "There are many ways to compute the DFT. The fastest method is the Fast Fourier Transform (FFT), which is an algorithm which computes the DFT. It is built into `numpy` as part of the `fft` submodule.\n",
    "\n",
    "If we look at the DFT coefficients of the two cosines we saw earlier, we can see that it is indeed doing exactly what we wanted it to: characterizing the frequency of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_cos_fft = np.fft.fft(slow_cos)\n",
    "fast_cos_fft = np.fft.fft(fast_cos)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.stem(n, np.abs(slow_cos_fft), use_line_collection=True)\n",
    "plt.title(\"$|DFT\\{\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)\\}|$\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"$|DFT\\{\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)\\}|$\")\n",
    "plt.stem(n, np.abs(fast_cos_fft), use_line_collection=True)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.stem(n, np.angle(slow_cos_fft), use_line_collection=True)\n",
    "plt.title(\"$\\\\arg \\\\left(DFT\\{\\cos\\\\left(\\\\frac{2\\pi}{100} n\\\\right)\\}\\\\right)$\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"$\\\\arg \\\\left(DFT\\{\\cos\\\\left(\\\\frac{10\\pi}{100} n\\\\right)\\}\\\\right)$\")\n",
    "plt.stem(n, np.angle(fast_cos_fft), use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-soundtrack",
   "metadata": {},
   "source": [
    "Since $\\cos\\left(\\frac{2\\pi}{100}n\\right) = \\frac{1}{2}\\left(e^{j\\frac{2\\pi}{100}n} + e^{-j\\frac{2\\pi}{100}n}\\right)$, we should expect peaks at $k = 1$ and $k =-1$ (note that because the roots of unity are periodic, $k=-1$ is the same basis vector as $k=99$). Likewise, since $\\cos\\left(\\frac{10\\pi}{100}n\\right) = \\frac{1}{2}\\left(e^{j\\frac{10\\pi}{100}n} + e^{-j\\frac{10\\pi}{100}n}\\right)$, we should expect peaks at $k=5$ and $k=-5$.\n",
    "\n",
    "There are a few things to note:\n",
    "1. The DFT coefficients are complex numbers, so we need both magnitude (top plots) and phase (bottom plots) to characterize the signal information\n",
    "2. For both $\\cos\\left(\\frac{2\\pi}{100}n\\right)$ and $\\cos\\left(\\frac{10\\pi}{100}n\\right)$, we should only expect 2 non-zero coefficients. However, we have apparently many non-zero coefficients. These are due to numerical instability in the FFT algorithm (if you print them out, these coefficients are on the order of $10^{-3}$ in magnitude and so are insignificant).\n",
    "3. The DFT basis is **not** orthonormal. This is why we must scale by $\\frac{1}{N}$ when applying the inverse DFT (`np.fft.ifft` in numpy). This is also why the peak magnitudes of the example signals above are 50 and not $\\frac{1}{2}$.\n",
    "4. DFT basis vectors are complex conjugates of each other (i.e $\\phi_k[n] = \\phi_{N-k}[n]^*$). This means for real signals, $X[k] = X^*[N-k]$.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-portsmouth",
   "metadata": {},
   "source": [
    "To get a better feel for the DFT, compute and plot the magnitude of the DFT coefficients of our clip from Tom's Diner in decibels ($dB = 20\\log_{10}(X[k])$). Since our song is a real signal, do not plot the complex conjugate coefficients since they are redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "song_dft = 20 * np.log10(np.abs(np.fft.fft(audio)))\n",
    "plt.plot(song_dft[:audio.size // 2]) # Coefficents N/2 to N are complex coefficients\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-memorabilia",
   "metadata": {},
   "source": [
    "**Comprehension Question**: Do you notice anything interesting about the chart above?\n",
    "\n",
    "**Answer**: Around index 150,000, there is a sharp decline in the magnitude of the DFT coefficients. It turns out that this DFT coefficient represents approximately 12.5 kHz (we'll see how to compute this later), which is close to the human hearing limit of about 20kHz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-canadian",
   "metadata": {},
   "source": [
    "**Comprehension Question**: What does the first coefficient $X[0]$ of the DFT represent in simple terms?\n",
    "\n",
    "**Answer**: It is the sum of the signal (we can see this from the formula by letting $k=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-salem",
   "metadata": {},
   "source": [
    "## II.a PSD\n",
    "\n",
    "In signal processing, due to noise, numerical stability, and other issues, we often care about the dominant frequencies in the signal (e.g when we are looking for formants in a vowel). This means we want to look at the magnitude of the DFT coefficients. However, sometimes peaks in the DFT are difficult to distinguish when looking at a magnitude plot. To better distinguish peaks, we can instead look at $|X[k]|^2$, the so-called **Power Spectral Density (PSD)**.\n",
    "\n",
    "The Power Spectral Density is the essentially the magnitude of the DFT of the auto-correlation of the signal $x$. This is because when $x[n]$ has DFT coefficients $X[k]$, then $x[-n]$ has DFT coefficients $X^*[k]$ and since auto-correlation is the convolution of $x[n] * x[-n]$, and convolution in the time-domain is multiplication in the frequency domain, $PSD = X[k] X^*[k] = |X[k]|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-adaptation",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the PSD to guess what vowels these are. Use the `n` argument of the DFT function to zero-pad the signal so that you compute the same number of DFT coefficients for each vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_1 = audio[int(7.3 * fs):int(fs * 7.325)] * np.hanning(0.025 * fs)\n",
    "vowel_2 = audio[int(11.6 * fs):int(11.625 * fs)] * np.hanning(0.025 * fs)\n",
    "freqs = np.linspace(0, fs, 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-dealer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "vowel_1_dft = np.abs(np.fft.fft(vowel_1, n=12000)) ** 2\n",
    "plt.plot(freqs[:800], vowel_1_dft[:800]) # Coefficents N/2 to N are complex coefficients\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# ** YOUR CODE HERE ** #\n",
    "vowel_2_dft = np.abs(np.fft.fft(vowel_2, n=12000)) ** 2\n",
    "plt.plot(freqs[:800], vowel_2_dft[:800]) # Coefficents N/2 to N are complex coefficients\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-third",
   "metadata": {},
   "source": [
    "Listen to the audio and see if you were right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(vowel_1, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(vowel_2, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-filter",
   "metadata": {},
   "source": [
    "# III. Frequency Domain Filtering - Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e609181",
   "metadata": {},
   "source": [
    "One really nice property of the DFT is that convolution in the time domain is equivalent to multiplication in the frequency domain, and convolution in frequency is equivalent to multiplication in time.\n",
    "- Important implications to sampling theory, which Anmol will cover in the next section\n",
    "- Makes convolution much more efficient: convolution in time on long signals of length n is $O(n^2)$, while converting to frequency domain, multiplying, and converting back is $O(n \\log{n})$\n",
    "- Makes it easy to control what frequencies you filter. If you want a high-pass filter with a specific cutoff for example, or a band-pass filter to only capture F0, you can design the filter in the frequency domain and then convert it back to the time domain!\n",
    "\n",
    "\n",
    "**The Frequency Response**\n",
    "\n",
    "Just like any signal in the time domain can be transformed into the frequency domain, so can every filter. For every impulse response $g(t)$ in the time domain, we can calculate $G(k)$ in the frequency domain by performing the DFT. Since multiplication in the frequency domain is equivalent to convolution in time, we can actually uniquely characterize any LTI filter $g$ by its impulse response or frequency response. Oftentimes, it is easier to design filters in the frequency domain, for example in speech recognition where the signal is easily separable by formant in the frequency domain, and we want to design a filter that captures speech while ignoring other frequencies. We don't much care what the shape of the filter is in the time domain, so we can design a filter in the frequency domain and impose restrictions on it to create the filter we need.\n",
    "\n",
    "**Types of Filters**\n",
    "\n",
    "Highpass filters let high frequencies through, while blocking low frequencies. This kind of filter is used to filter out low frequency background interference from power lines or machinery.\n",
    "\n",
    "Lowpass filters let low frequencies through, while blocking high frequencies. This kind of filter is often used to reduce high frequency noise, or eliminate aliasing from downsampling.\n",
    "\n",
    "Bandpass/bandstop filters pass or block only a specific range of frequencies. This might be useful in audio processing for focusing solely on F1 or F2, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbac431",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fft = np.fft.fft(audio)\n",
    "len_clip = audio.shape[0]\n",
    "cutoff_frequency = 500 * len_clip/fs\n",
    "\n",
    "freq_filter = np.array([1 if n < cutoff_frequency else 0 for n in range(len_clip)])\n",
    "time_filter = np.real(np.fft.ifft(freq_filter))\n",
    "audio_fft = np.fft.fft(audio)\n",
    "audio_filtered = np.real(np.fft.ifft(audio_fft*freq_filter))\n",
    "\n",
    "plt.plot(audio[:1000])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(freq_filter[:44100], color='r')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(audio_filtered[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4edb5ba",
   "metadata": {},
   "source": [
    "We can also design filters in the frequency domain to optimize for specific conditions, rather than simply using a box filter in the frequency domain (which has drawbacks, as we'll see later). One example of such a filter is the Butterworth Filter, which is designed to minimize variation in the frequency response in the passband, and thus avoid distortions in the output signal.\n",
    "\n",
    "The Butterworth filter has 3 main parameters:\n",
    "\n",
    "N, the order of the filter: how sharp the cutoff is\n",
    "\n",
    "Wh, the cutoff frequency: the frequency at which the frequency response drops to $1/\\sqrt{2}$ of the passband response\n",
    "\n",
    "btype, the type of filter (ie, lowpass, highpass, bandpass, bandstop)\n",
    "\n",
    "Documentation can be found at: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html#scipy.signal.butter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = signal.butter(2, 500, 'hp', fs=fs, output='sos')\n",
    "audio_filtered = signal.sosfilt(sos, audio)\n",
    "b, a = signal.butter(2, 500, 'low', analog=True)\n",
    "w, h = signal.freqs(b, a)\n",
    "plt.semilogx(w, 20 * np.log10(abs(h)), color='r')\n",
    "plt.title('Butterworth filter frequency response')\n",
    "plt.xlabel('Frequency [radians / second]')\n",
    "plt.ylabel('Amplitude [dB]')\n",
    "plt.margins(0, 0.1)\n",
    "plt.grid(which='both', axis='both')\n",
    "plt.axvline(500, color='green') # cutoff frequency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_filtered, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-andrew",
   "metadata": {},
   "source": [
    "# IV. Sampling Theory - Anmol\n",
    "\n",
    "In the real-world, most signals are continuous (i.e they are functions from $\\mathbb{R}\\to\\mathbb{R}$). Meanwhile, computers operate in the discrete space (i.e they are functions from $\\mathbb{N}\\to\\mathbb{R}$. This means that in order to analyze any continuous signal, we need to somehow discretize it so it can be stored in finite memory.\n",
    "\n",
    "Given a continuous signal $x_c(t)$, we can obtain a discrete signal by letting $x_d[n] = x_c(f(n))$ where $f: \\mathbb{N}\\to\\mathbb{R}$ describes our sampling scheme.\n",
    "\n",
    "A **uniform, non-adaptive sampling** scheme is where we pick some sampling frequency $\\omega_s$ and let $f(n) = \\frac{n}{\\omega_s}$. We can think of it as \"saving\" the value of the continuous time signal every $\\frac{1}{\\omega_s}$ seconds. _Uniform_ means that $\\omega_s$ is constant (i.e it does not depend on $n$), and _non-adaptive_ means $\\omega_s$ is independent of the samples we have seen so far. Uniform, non-adaptive sampling schemes are what we most frequently use for sampling because of their simplicity and well-known theoeretical guarantees. For the rest of the notebook, we will assume all sampling is uniform and non-adaptive.\n",
    "\n",
    "Because sampling has the potential to destroy information, we need to understand how it impacts the frequency domain. In continuous time, frequencies exist on the range $[0, \\infty)$. However, in discrete time, the fastest that a signal can change is $\\pi$ radians / sample (i.e alternating from 1 to -1 like $\\cos(\\pi n)$). When we take the DFT of a signal that we sampled, we want to know how our angular frequencies relate to the continuous frequencies.\n",
    "\n",
    "The easiest way to think of how continuous frequencies relate to discrete frequencies is by mapping the interval $\\left[0, \\frac{f_s}{2}\\right]$ (continuous frequencies) to the interval $[0, \\pi]$ (angular frequencies). Given an angular frequency $\\omega_d\\in[0, \\pi]$, the continuous frequency that it represent $\\omega_c = \\frac{f_s}{2\\pi}\\omega_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-latitude",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Plot the magnitude of DFT coefficients (in decibels) of our clip from Tom's Diner and label the x-axis with the continuous time frequencies. Ignore the complex conjugate coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# ** YOUR CODE HERE ** #\n",
    "freqs = np.linspace(0, fs / 2, audio.size // 2)\n",
    "song_dft = 20 * np.log10(np.abs(np.fft.fft(audio)))\n",
    "plt.plot(freqs, song_dft[:audio.size // 2]) # Coefficents N/2 to N are complex coefficients\n",
    "plt.xlabel(\"Hz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-mambo",
   "metadata": {},
   "source": [
    "## IV.a Aliasing\n",
    "\n",
    "How frequently we sample matters a lot. If we sample too slowly, then we lose information. If we sample too fast, then we are wasting memory. The three plots below are all samples of a 10 second long sine wave $x(t) = \\sin(2\\pi t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "hundred_hz = np.linspace(0, 10, 1000)\n",
    "ten_hz = np.linspace(0, 10, 100)\n",
    "one_hz = np.linspace(0, 10, 10)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(one_hz, np.sin(2 * np.pi * one_hz))\n",
    "plt.title(\"$f_s$ = 1Hz\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ten_hz, np.sin(2 * np.pi * ten_hz))\n",
    "plt.title(\"$f_s$ = 10Hz\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(hundred_hz, np.sin(2 * np.pi * hundred_hz))\n",
    "plt.title(\"$f_s$ = 100Hz\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-platform",
   "metadata": {},
   "source": [
    "Notice how the faster sampling frequencies 10Hz and 100Hz look virtually identical and cycle 10 times in 10 seconds as we expect a 1Hz sine wave to do. However, when we sample at 1Hz, our samples look like they came from a 0.1Hz sine wave, not a 1Hz sine wave. When higher frequencies \"masquerade\" as lower frequencies, this is known as **aliasing**. The effects of aliasing are very clear in the frequency domain through the following example where we sample the signal $x_c(t) = \\sin(2\\pi t) + \\sin(2\\pi * 10t)$ with a sampling frequency of 11Hz vs a sampling frequency of 50Hz vs a sampling frequency of 1000Hz over the course of 1 second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-riding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def x_c(t):\n",
    "    return np.sin(2 * np.pi * t) + np.sin(2 * np.pi * 10 * t)\n",
    "\n",
    "eleven_hz = np.linspace(0, 1, 11)\n",
    "fifty_hz = np.linspace(0, 1, 50)\n",
    "thousand_hz = np.linspace(0, 1, 1000)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(eleven_hz, x_c(eleven_hz))\n",
    "plt.title(\"$f_s$ = 11Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(fifty_hz, x_c(fifty_hz))\n",
    "plt.title(\"$f_s$ = 50Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(thousand_hz, x_c(thousand_hz))\n",
    "plt.title(\"$f_s$ = 1000Hz (Time Domain)\")\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(np.linspace(0, 11, eleven_hz.size), np.abs(np.fft.fft(x_c(eleven_hz))))\n",
    "plt.title(\"$f_s$ = 11Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(np.linspace(0, 50, fifty_hz.size), np.abs(np.fft.fft(x_c(fifty_hz))))\n",
    "plt.title(\"$f_s$ = 50Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(np.linspace(0, 1000, thousand_hz.size), np.abs(np.fft.fft(x_c(thousand_hz))))\n",
    "plt.title(\"$f_s$ = 1000Hz (Frequency Domain)\")\n",
    "plt.xlabel(\"Hz\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-inside",
   "metadata": {},
   "source": [
    "When we sampled at 50Hz, we had 2 very clear frequencies in our spectrum. However, at 11Hz, the second peak disappeared entirely! We can think of it as \"hiding\" in the 1Hz peak in the spectrum. At 1000Hz, we can measure a much larger range of frequencies, and so all of our peaks remain in the plot (they look squished together due to the scale of the axis).\n",
    "\n",
    "The **Nyquist Theorem** tells us how fast we need to sample in order to prevent aliasing. It states that in order to avoid aliasing, our sampling frequency $f_s$ must be at least twice the highest frequency present in the signal ($f_s > 2 * f_{max}$). In practice, due to noise, there is no maximum frequency of the signal, so we always have some aliasing. This can be minimized by using an analog anti-aliasing filter before we sample. Note that the Nyquist theorem holds in discrete time as well. Namely, if we want to downsample a recording, then the most we can sample is by a factor of $M$ (i.e take every Mth sample) such that $\\frac{\\pi}{M} > 2 * \\omega_{max}$.\n",
    "\n",
    "### Exercise\n",
    "How much can we downsample our audio clip before aliasing starts to degrade our audio quality? Which parts of the audio degrade first (hint, think about which frequencies are masked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_downsampled = audio[::2]\n",
    "ipd.Audio(two_downsampled, rate=fs // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_downsampled = audio[::4]\n",
    "ipd.Audio(four_downsampled, rate=fs // 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_downsampled = audio[::8]\n",
    "ipd.Audio(eight_downsampled, rate=fs // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixteen_downsampled = audio[::16]\n",
    "ipd.Audio(sixteen_downsampled, rate=fs // 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-necessity",
   "metadata": {},
   "source": [
    "## IV.b Quantization\n",
    "\n",
    "Earlier, we allowed our discrete signals to be functions from $\\mathbb{N}\\to\\mathbb{R}$. In words, we discretized time, but our signal took values over a continuous range. This is not entirely accurate since computers require use bits to represent numbers, so if we use $B$ bits to represent the values our signal takes on, we can only represent $2^B$ possible values.\n",
    "\n",
    "### Exercise\n",
    "See how changing the number of bits we use to represent audio impacts the quality of the audio (currently using 16bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio // 4096, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-parks",
   "metadata": {},
   "source": [
    "# V. Spectral Analysis - Hoang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-voice",
   "metadata": {},
   "source": [
    "## V.a Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87212bd6",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "* We can only capture a finite length of a signal\n",
    "* Impossible to capture an infinitely long signal (x[n] from $n = -\\infty$ to $n = \\infty$\n",
    "\n",
    "**How?**\n",
    "* Time domain: Multiple the signal x[n] with a window: $x[n] \\cdot w[n]$\n",
    "* Frequency domain: Convolution between the spectrum and the DTFT of the window, thus blurring the spectrum\n",
    "\n",
    "**Popular Windows**\n",
    "* Rectangular (never use this due to excessive sidelobe leakage)\n",
    "* Hann\n",
    "* Hamming\n",
    "* Tukey\n",
    "* Blackman\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-exhaust",
   "metadata": {},
   "source": [
    "## V.b STFT/Spectrogram\n",
    "\n",
    "**Overview**\n",
    "\n",
    "<img src=\"./images/STFT_steps.png\" alt=\"Step-by-step to perform STFT\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c961a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio signal attributes\n",
    "N = audio.size\n",
    "Tmax = N/fs\n",
    "print(f\"Sampling rate: {fs} Hz\")\n",
    "print(f\"Number of samples: {N} samples\")\n",
    "print(f\"Duration of recording: {Tmax} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Parameters\n",
    "Tw_s = 1.0 # window duration (seconds)\n",
    "s_s = 0.01 # step size (seconds)\n",
    "M = 4096 # number of frequencies (number of zeropadding)\n",
    "L = 296 # length of autocorrelation window (samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3809c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = np.arange(N)\n",
    "t = np.linspace(0, Tmax, N)\n",
    "Tw = int(np.round(Tw_s * fs)) # window duration (samples)\n",
    "s = int(np.round(s_s * fs)) # step size (samples)\n",
    "nS = int(np.ceil((N-Tw)/s)) # number of steps\n",
    "\n",
    "print(f\"Data window duration: {Tw} (samples)\")\n",
    "print(f\"Step size: {s} (samples)\")\n",
    "print(f\"Number of steps: {nS} (steps)\")\n",
    "\n",
    "# Divide the signal into segments of length Tw (window duration)\n",
    "audio_segs = []\n",
    "for idx in [int(i) for i in np.arange(0, N-Tw, s)]:\n",
    "    audio_segs.append(audio[idx:idx+Tw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(x, fs, ml):\n",
    "    '''\n",
    "        x (numpy.ndarray): time-domain signal\n",
    "        fs (float): sampling rate (Hz)\n",
    "        ml (int): maximum lag (samples)\n",
    "    '''\n",
    "    nx = len(x) # number of samples in signal\n",
    "    ml = min(ml, nx) # maximum lag\n",
    "    mx = np.mean(x) # mean of signal\n",
    "    vx = np.var(x) # variance of signal\n",
    "    x = x - mx; # remove the mean (DC component) from the signal\n",
    "\n",
    "    nz = int(2**np.ceil(np.log2(np.abs(2*nx-1)))) # number of zeropadding\n",
    "    X  = np.fft.fft(x, nz)\n",
    "    xc = np.fft.ifft(np.abs(X)**2)\n",
    "    ac = np.real(xc[1:ml])\n",
    "    ac = ac/nx # biased form of autocorrelation estimate\n",
    "    ac = ac/ac[0]\n",
    "\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a548a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram\n",
    "k = np.arange(0, M)\n",
    "we = k * (2*np.pi/M) # angular frequency of estimate (rad/s)\n",
    "fe = k * fs/M # frequency of estimate (Hz)\n",
    "wd = np.blackman(Tw) # data window\n",
    "wa = np.blackman(L) # autocorrelation window\n",
    "PSD_segs = []\n",
    "for idx in [int(i) for i in np.arange(0, nS)]:\n",
    "    # Window the signal then compute the autocorrelation\n",
    "    r_x_tmp = autocorrelation(audio_segs[idx] * wd, fs, Tw+1)\n",
    "    r_x = np.concatenate((np.flip(r_x_tmp[1:-1]), r_x_tmp))\n",
    "    \n",
    "    # Window the autocorrelation\n",
    "    if L >= len(r_x):\n",
    "        r_v = r_x * wa[int(abs(L-len(r_x))/2) : int(len(r_x)-1-abs(L-len(r_x))/2)];\n",
    "    else:\n",
    "        # TODO: debug here\n",
    "        r_v = wa * r_x[int(abs(L-len(r_x))/2) : int(len(r_x)-1-abs(L-len(r_x))/2)];\n",
    "\n",
    "    # Compute the PSD\n",
    "    PSD_segs.append(np.exp(-1j * we * 1) * np.fft.fft(r_v, M))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-intermediate",
   "metadata": {},
   "source": [
    "## V.c Spectral Estimation Methods\n",
    " \n",
    "**Periodogram**\n",
    "* Has excessive variance\n",
    "\n",
    "**Blackman-Tukey**\n",
    "* Reduce variance by smoothing the periodogram\n",
    "\n",
    "**Welch-Bartlett**\n",
    "* Reduce variance by averaging the periodogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-samba",
   "metadata": {},
   "source": [
    "## V.c Tradeoffs when performing spectral analysis\n",
    "\n",
    "**Overview**\n",
    "\n",
    "|      Spectrogram Design Parameters     |                              Tradeoffs                             |\n",
    "|:--------------------------------------:|:------------------------------------------------------------------:|\n",
    "|               Window types             |      Frequency resolution (mainlobe width) vs. Sidelobe leakage    |\n",
    "|           Data window duration         |     Frequency resolution vs. Time resolution; Bias vs. Variance    |\n",
    "|     Autocorrelation window duration    |                          Bias vs. Variance                         |\n",
    "|            Step size/Overlap           |                   Computation power vs. Resolution                 |\n",
    "\n",
    "**Data window duration tradeoff**\n",
    "\n",
    "|     Data Window Duration    |     Frequency Resolution    |     Time Resolution    |     PSD Bias    |     PSD Variance    |\n",
    "|:---------------------------:|:---------------------------:|:----------------------:|:---------------:|:-------------------:|\n",
    "|             Long            |             High            |           Low          |        Low      |         High        |\n",
    "|             Short           |              Low            |           High         |       High      |          Low        |\n",
    "\n",
    "**Autocorrelation window duration tradeoff**\n",
    "\n",
    "<img src=\"./images/autocorrelation_window_duration_tradeoff.png\" alt=\"Autocorrelation window duration tradeoff\" width=\"250\"/>\n",
    "\n",
    "**Step size/Overlap tradeoff**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-threshold",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. [Anmol's Course Notes from EE120 (Signals and Systems)](https://aparande.gitbook.io/berkeley-notes/ee120-0)\n",
    "2. [Anmol's Course Notes from EE123 (Digital Signal Processing)](https://aparande.gitbook.io/berkeley-notes/ee123-0)\n",
    "3. [Discrete Time Signal Formula Sheet](https://anmolparande.com/resources/berkeley/discrete-formula-sheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-italic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
